### RL学习框架设计思路

为巩固RL算法的掌握程度，所以我打算写一个学习框架。测试自己写的算法是否正确以及观看算法的各种指标。

既巩固了所学理论，又弥补了实战编程经验的不足，也会以后算法创新的仿真测试做了铺垫。所以我认为写这么一个学习框架性价比很高。

这是框架的设计文档，等框架全部完善后，会开源到github上。方便大家学习使用。

我计划实现两大部分，第一部分是实现各个算法的正确收敛以及各种指标的正确可视化。

第二部分是在第一部分正确的基础上，将std代码封装起来。然后针对每一种算法引入一个测试模块，生成不同测试点测试写的代码是否正确。

---

根据bellman-equation：

$$
v_\pi(s) = \sum_{a}\pi(a|s)\left[ \sum_{r}p(r|s,a)r + \gamma\sum_{s'}p(s'|s,a)v_\pi(s') \right]
$$

RL的目的，就是选取恰当的策略$\pi(a|s)$，使得$v_\pi(s) / q_\pi(s,a)$最大。

所有RL algorithm无非分为两类：

1. model-based：已知$p(r|s,a), p(s'|s,a)$
2. model-free：未知$p(r|s,a), p(s'|s,a)$



#### 环境设置

网格世界的参数：

1. n、m大小
2. 障碍物占总格子的比率、障碍物的位置、障碍物reward
3. 起点位置
4. 奖励位置、奖励reward
5. 是否固定随机种子

网格世界的要求：

1. 起点不能在障碍物内

网格世界的交互：

1. 生成完网格世界后，返回$p(r|s,a), p(s'|s,a)$
2. 实时接收到算法传入的policy后，绘制概率箭头
3. 实时接收到算法传入的state value后，绘制state value
4. 实时接收到算法传入的state value变化差距后，等结束后绘制自身state value变化差距的收敛图——看算法自身是否收敛
5. 接收算法传入的最终state value，绘制与标准state value之间差距的收敛图——看算法是否达到了$v^*$



#### 算法设置

算法的参数：

1. 迭代次数

算法的回调

1. 每一步迭代后，返回更新的policy
2. 每一步迭代后，返回更新的state value
3. 每一步迭代后，返回自身state value变化差距
4. 算法结束后，返回最终的state values



#### Model-based

1. 值迭代算法

$$
\begin{align*}
&\textbf{Initialization: }\text{The probability model $p(r|s,a)$ and $p(s'|s,a)$ for all $(s,a)$ are known. Initial guess $v_0$.} \\
&\text{At time $k$, do} \\
&\quad\quad \text{For every state $s \in \mathcal{S}$, do} \\
&\quad\quad\quad\quad \text{q-value: $q_k(s,a)=\sum_{r}p(r|s,a)r + \gamma \sum_{s'}p(s'|s,a)v_k(s')$} \\
&\quad\quad \text{Maximum action value: $a_k^*(s) = \text{argmax}_{a}q_k(a,s)$} \\
&\quad\quad \text{Policy update: $\pi_{k+1}(a|s)=1$ if $a=a_k^*(s)$, and $\pi_{k+1}(a|s)=0$ otherwise} \\
&\quad\quad \text{Value update: $v_{k+1}=q_k(a_k^*(s), s)$}
\end{align*}
$$

2. 策略迭代算法

$$
\begin{align*}
&\textbf{Initialization: }\text{The probability model $p(r|s,a)$ and $p(s'|s,a)$ for all $(s,a)$ are known. Initial guess $\pi_0$.} \\
&\text{At time $k$, do} \\
&\quad\quad \text{Policy evaluation:} \\
&\quad\quad \text{Initialization: an arbitrary initial guess $v_{\pi_k}^{(0)}$} \\
&\quad\quad \text{While $v_{\pi_k}^{(j)}$ has not converged, for the $j$th iteration, do} \\
&\quad\quad\quad\quad \text{For every state $s \in \mathcal{S}$, do} \\
&\quad\quad\quad\quad\quad\quad v_{\pi_k}^{(j)}(s) = \sum_{a}\pi_k(a|s)\left[ \sum_{r}p(r|s,a)r + \gamma\sum_{s'}p(s'|s,a)v_{\pi_k}^{(j)}(s') \right] \\
&\quad\quad \text{Policy improvement:} \\
&\quad\quad \text{For every state $s \in \mathcal{S}$, do} \\
&\quad\quad\quad\quad q_{\pi_k}(s,a) = \sum_{r}p(r|s,a)r + \gamma\sum_{s'}p(s'|s,a)v_{\pi_k}(s') \\
&\quad\quad a_k^*(s) = \text{argmax}_{a}q_{\pi_k}(s,a) \\
&\quad\quad \text{$\pi_{k+1}(a|s) = 1$ if $a = a_k^*(s)$, and $\pi_{k+1}(a|s)=0$ otherwise}
\end{align*}
$$



#### Model-free

1. MC policy

$$
\begin{align*}
&\textbf{Initialization: }\text{Initial guess $\pi_0$.} \\
&\text{At time $k$, do} \\
&\quad\quad \text{For every state $s \in \mathcal{S}$, do} \\
&\quad\quad\quad\quad \text{For every action $a \in \mathcal{A}(s)$, do} \\
&\quad\quad\quad\quad\quad\quad \text{Collect sufficiently many episodes starting from $(s,a)$ following $\pi_k$} \\
&\quad\quad\quad\quad\quad\quad \text{$q_{\pi_k}(s,a)=$ average return of all the episodes starting from $(s,a)$} \\
&\quad\quad\quad\quad \text{Policy improvement step:} \\
&\quad\quad\quad\quad a_k^*(s) = \text{argmax}_{a}q_{\pi_k}(s,a) \\
&\quad\quad\quad\quad \text{$\pi_{k+1}(a|s)=1$ if $a=a_k^*()s$, and $\pi_{k+1}(a|s)=0$ otherwise}
\end{align*}
$$

2. MC Exploring Starts

$$
\begin{align*}
&\textbf{Initialization: }\text{ Initial policy $\pi_0(a|s)$ and initial value $q(s,a)$ for all $(s,a)$.} \\
&\quad\quad\quad\quad\quad\quad\quad~\text{Returns(s,a) = 0 and Num(s,a) = 0 for all $(s,a)$.} \\
&\text{For each episode, do} \\
&\quad\quad \text{Episode generation: Select a starting state-action pair $(s_0, a_0)$}  \\
&\quad\quad \text{and ensure that all pairs can be possibly selected (this is the exploring-starts condition).} \\
&\quad\quad \text{Following the current policy, generate an episode of length $T$: $s_0, a_0, r_1, \cdots, s_{T-1}, a_{T-1}, r_T$.} \\
&\quad\quad \text{Initialization for each episode: $g \gets 0$} \\
&\quad\quad \text{For each step of the episode, $t = T - 1, T - 2, \cdots, 0, $ do} \\
&\quad\quad\quad\quad g \gets \gamma g + r_{t+1} \\
&\quad\quad\quad\quad \text{Returns($s_t, a_t$) $\gets$ Returns($s_t, a_t$) + $g$} \\
&\quad\quad\quad\quad \text{Policy evaluation:} \\
&\quad\quad\quad\quad q(s_t, a_t) \gets \text{Returns($s_t, a_t$) / Num($s_t,a_t$)} \\
&\quad\quad\quad\quad \text{Policy improvement:} \\
&\quad\quad\quad\quad \text{$\pi(a|s_t)=1$ if $a = \text{argmax}_aq(s_t, a)$ and $\pi(a|s_t)=0$ otherwise}
\end{align*}
$$

3. MC $\varepsilon$-Greedy

$$
\begin{align*}
&\textbf{Initialization: } \text{Initial policy $\pi_0(a|s)$ and initial value $q(s,a)$ for all $(s,a)$. Returns(s,a)=0 and Num(s,a)=0} \\
&\text{for all $(s,a)$. $\varepsilon \in (0, 1]$} \\
&\text{For each episode, do} \\
&\quad\quad \text{Episode generation: Select a starting state-action pair $(s_0, a_0)$. Following the current policy,} \\
&\quad\quad \text{generate an episode of length $T: s_0, a_0, r_1, \cdots, s_{T-1}, a_{T-1}, r_T$.} \\
&\quad\quad \text{Initialization for each episode: $g \gets 0$} \\
&\quad\quad \text{For each step of the episode, $t = T-1, T-2, \cdots, 0,$ do} \\
&\quad\quad\quad\quad g \gets \gamma g + r_{t+1} \\
&\quad\quad\quad\quad \text{Returns($s_t, a_t$) $\gets$ Returns($s_t, a_t$) + $g$} \\
&\quad\quad\quad\quad \text{Num($s_t, a_t$) $\gets$ Num($s_t, a_t$) + 1} \\
&\quad\quad\quad\quad \text{Policy evaluation:} \\
&\quad\quad\quad\quad q(s_t, a_t) \gets \text{Returns($s_t, a_t$) / Num($s_t, a_t$)} \\
&\quad\quad\quad\quad \text{Let } a^* = \text{argmax}_a q(s_t, a) \text{ and} \\
&\quad\quad\quad\quad\quad\quad \pi(a|s_t) = \begin{cases}1 - \frac{|\mathcal{A}(s_t)|-1}{|\mathcal{A}|}\varepsilon, \quad a = a^* \\ \frac{1}{|\mathcal{A}(s_t)|}\varepsilon, \quad a \ne a^* \end{cases}
\end{align*}
$$

4. TD

$$
\begin{cases}
v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t)\left[ v_t(s_t) - \left[ r_{t+1} + \gamma v_t(s_{t+1}) \right] \right], \\
v_{t+1}(s) = v_t(s), \forall s \ne s_t.
\end{cases}
$$

5. Sarsa

$$
\begin{cases}
q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t, a_t) - [r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})] \right], \\
q_{t+1}(s,a) = q_t(s,a), \quad \forall (s,a) \ne (s_t, a_t)
\end{cases}
$$

6. Q-learning

   - On-policy version:

   $$
   \begin{align*}
   &\text{For each episode, do} \\
   &\quad\quad \text{If the current $s_t$ is not the target state, do} \\
   &\quad\quad\quad\quad \text{Collect the experience $(s_t, a_t, r_{t+1}, s_{t+1})$: In particular, take action $a_t$ follwing} \\
   &\quad\quad\quad\quad \pi_t(s_t), \text{ generate } r_{t+1}, s_{t+1}. \\
   &\quad\quad\quad\quad \text{Update q-value:} \\
   &\quad\quad\quad\quad\quad\quad q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t, a_t) - \left[ r_{t+1} + \gamma \max_{a}q_t(s_{t+1}, a) \right] \right] \\
   &\quad\quad\quad\quad \text{Update policy:} \\
   &\quad\quad\quad\quad\quad\quad \pi_{t+1}(a|s_t) = 1 - \frac{\varepsilon}{|\mathcal{A}|}(|\mathcal{A}| - 1) \text{ if } a = \text{argmax}_a q_{t+1}(s_t, a) \\
   &\quad\quad\quad\quad\quad \quad \pi_{t+1}(a|s_t) = \frac{\varepsilon}{|\mathcal{A}|} \text{ otherwise}
   \end{align*}
   $$

   

   - Off-policy version

   $$
   \begin{align*}
   &\text{For each episode $\{s_0, a_0, r_1, s_1, a_1, r_2, \cdots \}$ generated by $\pi_b$, do} \\
   &\quad\quad \text{For each step $t = 0,1,2,\cdots$ of the episode, do} \\
   &\quad\quad\quad\quad \text{Update q-value:} \\
   &\quad\quad\quad\quad\quad\quad q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t, a_t) - \left[ r_{t+1} + \gamma \max_{a}q_t(s_{t+1}, a) \right] \right] \\
   &\quad\quad\quad\quad \text{Update target policy:} \\
   &\quad\quad\quad\quad\quad\quad \pi_{T,t+1}(a|s_t) = 1 \text{ of } a = \text{argmax}_a q_{t+1}(s_t, a) \\
   &\quad\quad\quad\quad\quad\quad \pi_{T,t+1}(a|s_t) = 0 \text{ otherwise}
   \end{align*}
   $$

7. TD-Linearer

$$
\begin{align*}
&\textbf{Initialization: } \text{A function $\hat v(s, w)$ that is a differentiable in $w$. Initial parameter $w_0$.} \\
&\text{For each episode generated following the polticy $\pi$, do} \\
&\quad\quad \text{For each step $(s_t, r_{t+1}, s_{t+1})$, do} \\
&\quad\quad\quad\quad w_{t+1} = w_t + \alpha_t[r_{t+1} + \gamma \hat v(s_{t+1}, w_t) - \hat v(s_t, w_t)] \cdot \nabla_w \hat v(s_t, w_t)
\end{align*}
$$

8. DQN

$$
\begin{align*}
&\text{Store the experience samples generated by $\pi_b$ in a replay buffer $\mathcal{B} = \{(s,a,r,s')\}$} \\
&\quad\quad \text{For each iteration, do} \\
&\quad\quad\quad\quad \text{Uniformly draw a mini-batch of samples from $\mathcal{B}$} \\
&\quad\quad\quad\quad \text{For each sample $(s,a,r,s')$, calculate the target values as $y_{\mathrm{T}} = r + \gamma \max_{a \in \mathcal{A}(s')}\hat q(s',a,w_{\mathrm{T}})$, where} \\
&\quad\quad\quad\quad \text{$w_\mathrm{T}$ is the parameter of the target network} \\
&\quad\quad\quad\quad \text{Update the main network to minimize $(y_\mathrm{T} - \hat q(s,a,w))^2$ using the mini-batch $\{(s,a,y_\mathrm{T})\}$} \\
&\quad\quad \text{Set $w_\mathrm{T} = w$ every $C$ iterations}
\end{align*}
$$

