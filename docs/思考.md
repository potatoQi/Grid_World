- 对于传统MDP框架, model-based型的算法：
    1. 值迭代
    2. 策略迭代
    - 要求：
      - 根据理解直接写出$v_\pi(s), q_\pi(s,a)$的表达式
      - 看着表达式能说出值迭代和策略迭代的算法具体实现思路
- 对于MC系列算法
    1. MC Basic
    2. MC Exploring Starts
    3. MC epsilon greedy
    - 要求：
      - 能把轨迹采样出来
      - 能立马说出其实就是在策略迭代上改的，只不过action value靠采样而已
      - 理解“探索率”的目的和影响
- 对于时序差分系列算法
    1. Sarsa
    2. Q-learning
    - 要求：
      - 能说出时序差分系列算法的本质（MC Exploring Starts的完全记忆化 / bellman-equation）
      - 能分别写出Sarsa和Q-learning的迭代式
- 对于值函数近似算法
    1. DQN
    - 要求：
      - 说出该算法的本质（深度学习的方法，拟合一个函数）
      - 说出此时“探索率”的本质（反而探索率越高越好，因为此时采样不是用来估计某个值，而是采集尽可能不同且多的数据，用来喂给网络。因为网络就是要做到在任何pairs下都要求能拟合）
      - 手搓出DQN_1、DQN_2锻炼码力，增加对pytorch框架的熟悉